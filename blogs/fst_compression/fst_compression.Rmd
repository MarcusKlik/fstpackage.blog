---
title: "Multi-threaded compression from R using LZ4 and ZSTD"
author: "Mark Klik"
date: '2017-12-16'
coverImage: /img/fst_compression/media/space_coast.jpg
editor_options:
  chunk_output_type: console
metaAlignment: center
slug: fst_compression
tags:
- fst package
- compression
thumbnailImage: /img/fst_compression/media/compression.jpg
thumbnailImagePosition: left
categories:
- R
- compression
- fst package
---

The _fst_ package uses LZ4 and ZSTD compression to compact data stored in the _fst_ format. And now you can also use these compressors directly with methods _compress_fst_ and _decompress_fst_.

```{r, results='asis', echo=FALSE}
cat("<!--more-->\n\n")
```

## LZ4 and ZSTD

[LZ4](http://lz4.github.io/lz4/) is one of the fastest compressors around, and like all LZ77-type compressors, decompression is even faster. The _fst_ package uses LZ4 to compress and decompress data at the lower compression level settings of _write__fst_. For higher compression levels, the [ZSTD](https://github.com/facebook/zstd) compressor is used. ZSTD also offers fast decompression but has higher compression ratio's than LZ4.

## Multi-threaded access to LZ4 and ZSTD compressors

From _fst_ version 0.8.0 these compressors can be used directly using methods _compress\_fst_ and _decompress\_fst_.

As an example, we download the _survey\_results\_public.csv_ file [from Kaggle](https://www.kaggle.com/stackoverflow/so-survey-2017) and recompress it using ZSTD:

```{r, echo = FALSE, results = 'hide'}
library(fst)
threads_fst(8)  # use 8 threads
```

```{r}
# file downloaded from https://www.kaggle.com/stackoverflow/so-survey-2017
sample_file <- "large/survey_results_public.csv"

# read file into a raw vector
raw_vec <- readBin(sample_file, "raw", file.size(sample_file))  # read byte contents 

# compress bytes with ZSTD (at a low setting)
compressed_vec <- compress_fst(raw_vec, "ZSTD", 10)

length(compressed_vec) / length(raw_vec)  # compression ratio
```

This compresses the contents of the _survey\_results\_public.csv_ file to about `r round(100 * length(compressed_vec) / length(raw_vec))` percent of the original size. You can decompress again with:

```{r}
raw_vec_decompressed <- decompress_fst(compressed_vec)
```

What's special about the _fst_ implementation is that it's a fully multi-threaded implementation of the underlying compression algorithms, boosting the compression and decompression speeds:

```{r, eval = FALSE, results = 'hide'}
library(microbenchmark)

threads_fst(8)  # use 8 threads

# measure ZSTD compression performance
compress_time <- microbenchmark(
  compress_fst(raw_vec, "ZSTD", 10)
)

# decompress again
decompress_time <- microbenchmark(
  decompress_fst(compressed_vec)
)

cat("Compress: ", 1e3 * as.numeric(object.size(raw_vec)) / median(compress_time$time),
    "Decompress: ", 1e3 * as.numeric(object.size(raw_vec)) / median(decompress_time$time))
```

So the data in this file can be compressed to `r round(100 * length(compressed_vec) / length(raw_vec))` percent of the original size with a compression speed of `r round(10 * as.numeric(object.size(raw_vec)) / median(compress_time$time)) / 10` GB/s, that's pretty fast!

## bring on the cores

With more cores, you can do more parallel compression work. With a small benchmark we can show this dependency:

```{r, eval = FALSE}
library(data.table)

# benchmark results
bench <- data.table(Threads = as.integer(NULL), Time = as.numeric(NULL),
  Mode = as.character(NULL), Level = as.integer(NULL), Size = as.numeric(NULL))

for (level in 10 * 0:5) {
  for (threads in 1:20) {

    cat(".")
    threads_fst(threads)
    compress_time <- microbenchmark(compressed_vec <- compress_fst(raw_vec, "ZSTD", level), times = 10)
    decompress_time <- microbenchmark(decompress_fst(compressed_vec), times = 10)
    
    bench <- rbindlist(list(bench, 
      data.table(
        Threads = threads,
        Time = median(compress_time$time),
        Mode = "Compress",
        Level = level,
        Size = as.integer(object.size(compressed_vec))),
      data.table(
        Threads = threads,
        Time = median(decompress_time$time),
        Mode = "Decompress",
        Level = level,
        Size = as.integer(object.size(compressed_vec)))))
  }
}

# saveRDS(bench, "comp_res.rds")  # smallest file
```

This creates a _data.table_ with compression and decompression benchmark results. We can display these results in a graph:

```{r, eval = FALSE}
library(ggplot2)

bench[, Speed := 1e3 * object.size(raw_vec) / Time]
bench[, Level := as.factor(Level)]

ggplot(bench) +
  geom_line(aes(Threads, Speed, colour = Level)) +
  geom_point(aes(Threads, Speed, colour = Level)) +
  facet_wrap(~Mode)
```

```{r, echo = FALSE}
library(ggplot2)

bench <- readRDS("comp_res.rds")
bench[, Speed := 1e3 * object.size(raw_vec) / Time]
bench[, Level := as.factor(Level)]

ggplot(bench) +
  geom_line(aes(Threads, Speed, colour = Level)) +
  geom_point(aes(Threads, Speed, colour = Level)) +
  facet_wrap(~Mode)
```


## Column compression in _fst_

Method _compress_fst_ feeds data directly to the LZ4 or ZSTD compressor. But when _fst_ compresses a column from an R _data.frame_, additional information is known about the data that has to be compressed. For example, when you compress a vector of _integer_ values, you already know that the data is organized in groups of 4 bytes (that's the in-memory representation of a single )

