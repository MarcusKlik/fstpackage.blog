---
title: "Multi-threaded LZ4 and ZSTD compression from R"
author: "Mark Klik"
date: '2017-12-16'
coverImage: /img/fst_compression/media/space_coast.jpg
editor_options:
  chunk_output_type: console
metaAlignment: center
slug: fst_compression
tags:
- fst package
- compression
thumbnailImage: /img/fst_compression/media/compression.jpg
thumbnailImagePosition: left
categories:
- R
- compression
- fst package
---

The _fst_ package uses LZ4 and ZSTD to compress columnar data stored in the _fst_ format. In the latest release, methods _compress\_fst_ and _decompress\_fst_ were added which allow for direct (multi-threaded) access to these excellent compressors.

```{r, results='asis', echo=FALSE}
cat("<!--more-->\n\n")
```

```{r, results='asis', echo=FALSE}
cat("<!-- toc -->\n\n")
```

# LZ4 and ZSTD

[LZ4](http://lz4.github.io/lz4/) is one of the fastest compressors around, and like all LZ77-type compressors, decompression is even faster. The _fst_ package uses LZ4 to compress and decompress data when lower compression levels are selected (in method _write\_fst_). For higher compression levels, the [ZSTD](https://github.com/facebook/zstd) compressor is used, which offers superior compression ratio's but requires more CPU resources.

# How to use LZ4 and ZSTD from the _fst_ package

In _fst_ version 0.8.0, methods _compress\_fst_ and _decompress\_fst_ were added. These methods give you direct access to LZ4 and ZSTD. As an example of how they can be used, we download a file [from Kaggle](https://www.kaggle.com/stackoverflow/so-survey-2017) (with a size of about 90 MB) and recompress it using ZSTD:

```{r, echo = FALSE, results = 'hide', message = FALSE}
library(fst)
threads_fst(8)  # use 8 threads
```

```{r}
library(fst)

# you can download this file from https://www.kaggle.com/stackoverflow/so-survey-2017
sample_file <- "survey_results_public.csv"

# read file contents into a raw vector
raw_vec <- readBin(sample_file, "raw", file.size(sample_file))

# compress bytes with ZSTD at a 20 percent compression level setting
compressed_vec <- compress_fst(raw_vec, "ZSTD", 20)

# write the compressed data into a new file
compressed_file <- "survey_results_public.fsc"
writeBin(compressed_vec, compressed_file)

# compression ratio
file.size(sample_file) / file.size(compressed_file)
```

The contents of the _survey\_results\_public.csv_ file are compressed to about `r round(100 * length(compressed_vec) / length(raw_vec))` percent of the original size (calculated as the inverse of the compression ratio). In this example a compression setting of 20 percent (of maximum) was used. To decompress that file again you can do:

```{r}
# read compressed file into a raw vector
compressed_vec <- readBin(compressed_file, "raw", file.size(compressed_file))

# decompress file contents
raw_vec_decompressed <- decompress_fst(compressed_vec)
```

And because _data.table_'s _fread_ method can parse in-memory data directly, you can even do:

```{r}
library(data.table)

# read a dataset from an in-memory csv
dt <- fread(rawToChar(raw_vec_decompressed))
```

effectively reading your _data.table_ from a compressed file (at pretty impressive speeds)!

# Multi-threading

Methods _compress\_fst_ and _decompress\_fst_ use a fully multi-threaded implementation of the LZ4 and ZSTD algorithms. This is accomplished by dividing the data into (at maximum) 48 blocks, which are then processed in parallel. This increases the compression and decompression speeds significantly at a small cost to compression ratio:

```{r, eval = FALSE, results = 'hide'}
library(microbenchmark)

threads_fst(8)  # use 8 threads

# measure ZSTD compression performance at low setting
compress_time <- microbenchmark(
  compress_fst(raw_vec, "ZSTD", 10),
  times = 500
)

# decompress again
decompress_time <- microbenchmark(
  decompress_fst(compressed_vec),
  times = 500
)

cat("Compress: ", 1e3 * as.numeric(object.size(raw_vec)) / median(compress_time$time), "MB/s",
    "Decompress: ", 1e3 * as.numeric(object.size(raw_vec)) / median(decompress_time$time), "MB/s")
```

```{r, echo = FALSE}
library(microbenchmark)
library(data.table)

bench <- readRDS("comp_res_laptop.rds")
measurement <- bench[Level == 10 & Threads == 8]

compress_speed <- as.numeric(object.size(raw_vec)) / measurement[Mode == "Compress", Time]
decompress_speed <- as.numeric(object.size(raw_vec)) / measurement[Mode == "Decompress", Time]

compresion_factor <- length(raw_vec) / measurement[, mean(Size)]  # compression ratio

cat("Compress: ", 1e3 * compress_speed, "MB/s",
    "Decompress: ", 1e3 * decompress_speed, "MB/s")
```

The contents of the _survey\_results\_public.csv_ file can be compressed with a factor of `r round(10 * compresion_factor) / 10` at a compression speed of around `r round(10 * compress_speed) / 10` GB/s!

# Bring on the cores

With more cores, you can do more parallel compression work. With a small benchmark we can show this dependency:

```{r, eval = FALSE}
library(data.table)

# benchmark results
bench <- data.table(Threads = as.integer(NULL), Time = as.numeric(NULL),
  Mode = as.character(NULL), Level = as.integer(NULL), Size = as.numeric(NULL))

# Note that compression time increases steadily with higher levels.
# If you want to run this code yourself, start by using levels 10 * 0:5
for (level in 10 * 0:10) {
  for (threads in 1:parallel::detectCores()) {

    cat(".")  # show some progress
    threads_fst(threads)  # set number of threads to use
    
    # compress and decompress measurements
    compress_time <- microbenchmark(
      compressed_vec <- compress_fst(raw_vec, compressor, level), times = 25)
    decompress_time <- microbenchmark(
      decompress_fst(compressed_vec), times = 25)
    
    # add measurements to the benchmark results
    bench <- rbindlist(list(bench, 
      data.table(
        Threads = threads,
        Time = median(compress_time$time),
        Mode = "Compress",
        Level = level,
        Size = as.integer(object.size(compressed_vec))),
      data.table(
        Threads = threads,
        Time = median(decompress_time$time),
        Mode = "Decompress",
        Level = level,
        Size = as.integer(object.size(compressed_vec)))))
  }
}
```

This creates a _data.table_ with compression and decompression benchmark results. We can display these results in a graph:

```{r, eval = FALSE}
library(ggplot2)

bench[, Speed := 1e3 * object.size(raw_vec) / Time]  # unit to MB/s
bench[, Level := as.factor(Level)]  # for display purposes

ggplot(bench) +
  geom_line(aes(Threads, Speed, colour = Level)) +
  geom_point(aes(Threads, Speed, colour = Level)) +
  facet_wrap(~Mode) +
  theme_minimal() +
  ylab("Speed (MB/s)")
```

```{r, echo = FALSE,  fig.path = "img/fig-", fig.width = 10, echo = FALSE}
library(ggplot2)

bench[, Speed := 1e3 * object.size(raw_vec) / Time]
bench[, Level := as.factor(Level)]

ggplot(bench) +
  geom_line(aes(Threads, Speed, colour = Level)) +
  geom_point(aes(Threads, Speed, colour = Level)) +
  facet_wrap(~Mode) +
  theme_minimal() +
  ylab("Speed (MB/s)")
```

As can be expected, the compression speed is highest for lower compression level settings. But interesting enough, decompression speeds actually increase with higher compression settings and decompression speeds of more than 3 GB/s were measured in our experiment!

The compression ratio measured for the selected levels are shown below (with LZ4 added as well):

```{r, echo = FALSE,  fig.path = "img/fig-", fig.width = 10, echo = FALSE}
bench_zstd <- bench[, .(Level, Size)][, Compressor := "ZSTD"]
bench_lz4 <- readRDS("comp_res_lz4.rds")[, .(Level, Size)][, Compressor := "LZ4"]

bench <- rbindlist(list(bench_zstd, bench_lz4))

bench[, Ratio := object.size(raw_vec) / Size]
bench[, Level := as.numeric(as.character(Level))]

ratios <- bench[, .(Ratio = median(Ratio)), by = "Level,Compressor"]

ggplot(ratios) +
  geom_line(aes(Level, Ratio)) +
  geom_point(aes(Level, Ratio)) +
  geom_point(data = ratios[Level == 20 & Compressor == "ZSTD"],
    aes(Level, Ratio), size = 5, shape = 1, colour = "darkgrey") +
  facet_wrap(~Compressor) +
  theme_minimal()
```

It's clear from the graph that with a combination of LZ4 and ZSTD, a wide range of compression ratio's (and speeds) is available to the user.

# The case for high compression levels

There are many use cases where you compress your data only once but decompress it much more often. For example, you can compress and store a file that will need to be read many times in the future. In that case it's very useful to spend the CPU resources on compressing at a higher setting. It will give you higher decompression speeds during reads and the compressed data will occupy less space.
